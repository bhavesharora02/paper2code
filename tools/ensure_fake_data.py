#!/usr/bin/env python3
# tools/ensure_fake_data.py
"""
Patch generated repos to guarantee a safe tiny data_loader for quick CI/smoke/metric runs.

Usage:
  python tools/ensure_fake_data.py --runs_dir runs/          # patch all run repos
  python tools/ensure_fake_data.py --run runs/cv_vit_xxx    # patch single run
  python tools/ensure_fake_data.py --runs_dir runs/ --dry-run

Behavior:
 - Backs up original files to <runs_dir>/_backups_fake_data/<run_name>/
 - Replaces repo/data_loader.py with a safe tiny data loader (if it doesn't already appear to be present)
 - Optionally adds a short USE_FAKE_DATA comment to trainer.py to document the env flags (non-destructive)
"""
from __future__ import annotations
import argparse
import shutil
import time
from pathlib import Path

SAFE_DATA_LOADER = r'''# Auto-generated safe data_loader for smoke/tests (generated by tools/ensure_fake_data.py)
# Honors env: USE_FAKE_DATA=1 or SCALED_RUN=1 to force tiny synthetic datasets.
import os
import torch
from torch.utils.data import Dataset, DataLoader

def _num_classes_safe(n):
    try:
        return max(1, int(n))
    except Exception:
        return 2

class TinyTextDataset(Dataset):
    def __init__(self, tokenizer=None, max_length=128, length=64, num_classes=2):
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.length = int(length)
        self.num_classes = _num_classes_safe(num_classes)
        self.data = [
            ("good movie great acting", 1),
            ("bad plot boring scenes", 0),
            ("excellent visuals and story", 1),
            ("terrible pacing and sound", 0),
        ]
        # replicate to reach desired length
        self.data = (self.data * ((self.length // len(self.data)) + 1))[: self.length]

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        text, y = self.data[idx]
        y = int(y) % max(1, self.num_classes)
        if self.tokenizer is None:
            # return a fake tensor matching many model embeddings (e.g. 768 dims)
            x = torch.randn(768)
            return {"inputs": x, "labels": torch.tensor(y, dtype=torch.long)}
        enc = self.tokenizer(text, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')
        enc = {k: v.squeeze(0) for k, v in enc.items()}
        return {"inputs": enc, "labels": torch.tensor(y, dtype=torch.long)}

class TinyImageDataset(Dataset):
    def __init__(self, size=224, num_classes=2, length=32):
        self.size = int(size)
        self.length = int(length)
        self.num_classes = _num_classes_safe(num_classes)

    def __len__(self):
        return self.length

    def __getitem__(self, idx):
        # return random image tensor [C,H,W]
        x = torch.rand(3, self.size, self.size)
        # safe randint defensive: ensure upper bound > lower bound
        if self.num_classes <= 0:
            y = 0
        else:
            try:
                y = torch.randint(0, self.num_classes, (1,)).item()
            except Exception:
                y = 0
        y = int(y) % max(1, self.num_classes)
        return {"inputs": x, "labels": torch.tensor(y, dtype=torch.long)}

def build_dataloaders(config_path: str = None):
    # Check env flags and choose tiny dataset sizes
    use_fake = os.environ.get("USE_FAKE_DATA") == "1" or os.environ.get("SCALED_RUN") == "1"
    batch_size = 4 if use_fake else 8
    # default to image tiny dataset
    train = TinyImageDataset(size=64 if use_fake else 224, num_classes=2, length=32 if use_fake else 128)
    val = TinyImageDataset(size=64 if use_fake else 224, num_classes=2, length=16 if use_fake else 64)
    train_loader = DataLoader(train, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val, batch_size=batch_size)
    return train_loader, val_loader
'''

TRAINER_HINT = r'''# NOTE: This repo honors small synthetic datasets when:
#   USE_FAKE_DATA=1 or SCALED_RUN=1
# You can run a quick smoke with:
#   USE_FAKE_DATA=1 python repo/trainer.py
'''

def backup_file(src: Path, backup_dir: Path):
    backup_dir.mkdir(parents=True, exist_ok=True)
    ts = int(time.time())
    if src.exists():
        dest = backup_dir / f"{src.name}.{ts}.orig"
        shutil.copy2(src, dest)
        return dest
    else:
        # create marker file
        dest = backup_dir / f"{src.name}.{ts}.missing"
        dest.write_text("(missing prior to patch)", encoding="utf-8")
        return dest

def patch_data_loader(repo: Path, backup_dir: Path, dry_run: bool = False) -> bool:
    """Return True if patched (or would patch in dry-run)."""
    target = repo / "data_loader.py"
    # Heuristic: don't replace if file already looks like a tiny data loader
    content = ""
    if target.exists():
        try:
            content = target.read_text(encoding="utf-8")
            if "TinyImageDataset" in content or "USE_FAKE_DATA" in content or "TinyTextDataset" in content:
                print(f"  skipping data_loader.py (looks like already safe): {repo}")
                return False
        except Exception:
            pass

    print(f"  patching data_loader.py -> {target}")
    if dry_run:
        return True
    # backup
    backup_file(target, backup_dir)
    target.write_text(SAFE_DATA_LOADER, encoding="utf-8")
    return True

def ensure_trainer_hint(repo: Path, backup_dir: Path, dry_run: bool = False) -> bool:
    trainer = repo / "trainer.py"
    if not trainer.exists():
        return False
    try:
        txt = trainer.read_text(encoding="utf-8")
    except Exception:
        txt = ""
    if "USE_FAKE_DATA" in txt or "SCALED_RUN" in txt:
        print(f"  trainer.py already references USE_FAKE_DATA/SCALED_RUN: {repo}")
        return False
    print(f"  inserting trainer hint into trainer.py -> {trainer}")
    if dry_run:
        return True
    backup_file(trainer, backup_dir)
    # prepend hint
    new = TRAINER_HINT + "\n" + txt
    trainer.write_text(new, encoding="utf-8")
    return True

def process_run(run_repo: Path, backup_root: Path, dry_run: bool = False):
    repo = run_repo
    run_name = repo.parent.name
    backup_dir = backup_root / run_name
    print(f"Processing repo: {repo}  (backup -> {backup_dir})")
    patched = False
    try:
        p1 = patch_data_loader(repo, backup_dir, dry_run=dry_run)
        p2 = ensure_trainer_hint(repo, backup_dir, dry_run=dry_run)
        patched = p1 or p2
    except Exception as e:
        print("  ERROR patching:", e)
    if patched:
        print("  -> patched")
    else:
        print("  -> nothing changed")
    return patched

def find_repos(runs_dir: Path):
    for p in sorted(runs_dir.glob("*_*")):
        repo = p / "repo"
        if repo.is_dir():
            yield repo

def main():
    ap = argparse.ArgumentParser()
    ap.add_argument("--runs_dir", default="runs/", help="directory with run folders")
    ap.add_argument("--run", help="single run dir (overrides runs_dir)")
    ap.add_argument("--backup_dir", default="runs/_backups_fake_data", help="where to store backups")
    ap.add_argument("--dry-run", action="store_true")
    ap.add_argument("--yes", action="store_true", help="don't ask for confirmation")
    args = ap.parse_args()

    runs_dir = Path(args.runs_dir)
    backup_root = Path(args.backup_dir)
    candidates = []
    if args.run:
        repo = Path(args.run) / "repo"
        if not repo.exists():
            print(f"Run repo not found: {repo}")
            raise SystemExit(2)
        candidates = [repo]
    else:
        candidates = list(find_repos(runs_dir))
        if not candidates:
            print("No run repos found under", runs_dir)
            raise SystemExit(0)

    print("Candidates to patch:", len(candidates))
    for r in candidates:
        print(" -", r.parent.name)

    if not args.yes and not args.dry_run:
        ans = input("Apply patches? (y/N) ")
        if ans.strip().lower() not in ("y", "yes"):
            print("Aborting")
            return

    for repo in candidates:
        process_run(repo, backup_root, dry_run=args.dry_run)

if __name__ == "__main__":
    main()

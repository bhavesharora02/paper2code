{% set mapping = mapping if mapping is defined else {} %}
{% set ir = ir if ir is defined else {} %}
{% set header_comment = header_comment if header_comment is defined else '' %}

# Generated from {{ run_dir }} â€” DO NOT EDIT by hand
import json
from pathlib import Path
import inspect

import torch

from model import model, loss_fn, optimizer
from data_loader import build_dataloaders


def _to_device(x, device):
    if isinstance(x, dict):
        return {k: _to_device(v, device) for k, v in x.items()}
    if hasattr(x, 'to'):
        return x.to(device)
    return x


def _model_call(m, inputs):
    """Call model for both Tensor and dict inputs, handling HF models."""
    if isinstance(inputs, dict):
        return m(**inputs)
    # Tensor path: if forward has 'pixel_values', pass as named arg
    try:
        sig = inspect.signature(m.forward)
        if 'pixel_values' in sig.parameters and isinstance(inputs, torch.Tensor):
            return m(pixel_values=inputs)
    except Exception:
        pass
    return m(inputs)


def train_one_epoch(model, loader, loss_fn, optimizer, device):
    model.train()
    total_loss = 0.0
    for batch in loader:
        batch = {k: _to_device(v, device) for k, v in batch.items()}
        inputs, labels = batch['inputs'], batch['labels']
        outputs = _model_call(model, inputs)
        logits = getattr(outputs, 'logits', outputs)
        loss = loss_fn(logits, labels)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        total_loss += float(loss.item())
    return total_loss / max(1, len(loader))


def evaluate(model, loader, loss_fn, device):
    model.eval()
    total_loss = 0.0
    total_correct = 0
    total = 0
    with torch.no_grad():
        for batch in loader:
            batch = {k: _to_device(v, device) for k, v in batch.items()}
            inputs, labels = batch['inputs'], batch['labels']
            outputs = _model_call(model, inputs)
            logits = getattr(outputs, 'logits', outputs)
            loss = loss_fn(logits, labels)
            preds = torch.argmax(logits, dim=-1)
            total_correct += (preds == labels).sum().item()
            total += labels.numel()
            total_loss += float(loss.item())
    acc = total_correct / max(1, total)
    return {"loss": total_loss / max(1, len(loader)), "accuracy": acc}


def main():
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)

    train_loader, val_loader = build_dataloaders()

    epochs = 1
    metrics = {}
    for ep in range(epochs):
        tr_loss = train_one_epoch(model, train_loader, loss_fn, optimizer, device)
        val = evaluate(model, val_loader, loss_fn, device)
        print(f"[train] epoch={ep+1} loss={tr_loss:.4f} val_loss={val['loss']:.4f} val_acc={val['accuracy']:.4f}")
        metrics = {"epoch": ep + 1, "train_loss": tr_loss, **val}

    # Write metrics.json beside this file
    out = Path(__file__).resolve().parent / 'metrics.json'
    out.write_text(json.dumps(metrics, indent=2))
    print(f"[trainer] wrote {out}")


if __name__ == "__main__":
    main()
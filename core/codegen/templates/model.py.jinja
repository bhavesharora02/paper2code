# Generated from {{ run_dir }} — DO NOT EDIT by hand
import torch

# Render constants at generation-time so generated module does not depend on runtime 'ir'
NUM_CLASSES = {{ ((ir.get('dataset') or {}).get('num_classes')) | default(2) | int }}
HIDDEN = {{ ((ir.get('hyperparameters') or {}).get('hidden_size')) | default(128) | int }}

# Try optional imports (mapping.imports may be a list of import lines)
try:
{% for line in (mapping.get('imports') or []) %}
    {{ line }}
{% else %}
    # no optional imports provided by mapping — keep a pass so try-block is non-empty
    pass
{% endfor %}
except Exception as e:
    print(f"[model] optional import failed: {e}")

# Build model from mapping if provided; else fallback to TinyFallback
model = None
try:
    {% if mapping.get('model_constructor') %}
    model = {{ mapping.get('model_constructor') }}
    {% else %}
    raise RuntimeError("no mapping constructor")
    {% endif %}
except Exception as e:
    print(f"[model] constructor failed: {e}")

    class TinyFallback(torch.nn.Module):
        def __init__(self, input_dim=768, hidden:int = HIDDEN, num_classes: int = NUM_CLASSES):
            super().__init__()
            self.hidden = int(hidden)
            self.num_classes = max(1, int(num_classes))
            self.fc1 = torch.nn.Linear(self.hidden, max(1, min(1024, self.hidden)))
            self.relu = torch.nn.ReLU()
            self.out = torch.nn.Linear(max(1, min(1024, self.hidden)), self.num_classes)
            self.flatten = torch.nn.Flatten(start_dim=1)

        def forward(self, x):
            if isinstance(x, dict):
                x = x.get('pixel_values') or next(iter(x.values()))
            if isinstance(x, torch.Tensor) and x.dim() > 2:
                x = self.flatten(x)
            # Ensure feature dim matches expected hidden; pad/trim if needed
            if isinstance(x, torch.Tensor) and x.dim() == 2:
                b, d = x.size(0), x.size(1)
                if d != self.hidden:
                    if d > self.hidden:
                        x = x[:, :self.hidden]
                    else:
                        pad = (0, self.hidden - d)
                        x = torch.nn.functional.pad(x, pad)
            x = self.relu(self.fc1(x))
            return self.out(x)

    model = TinyFallback()

# Loss & optimizer fallbacks
try:
    loss_fn = {{ mapping.get('loss_constructor') or "torch.nn.CrossEntropyLoss()" }}
except Exception:
    loss_fn = torch.nn.CrossEntropyLoss()

try:
    optimizer = {{ mapping.get('optimizer_constructor') or "torch.optim.Adam(model.parameters(), lr=1e-3)" }}
except Exception:
    try:
        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
    except Exception:
        optimizer = None

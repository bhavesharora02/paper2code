# Generated from {{ run_dir }} â€” DO NOT EDIT by hand
import torch
from torch.utils.data import Dataset, DataLoader

# NUM_CLASSES rendered at generation time to avoid runtime dependency on `ir`
NUM_CLASSES = {{ ((ir.get('dataset') or {}).get('num_classes')) | default(2) | int }}

try:
    from transformers import AutoTokenizer
except Exception:
    AutoTokenizer = None

class TinyTextDataset(Dataset):
    def __init__(self, tokenizer=None, max_length=128, length=64, num_classes: int = NUM_CLASSES):
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.length = int(length)
        # ensure at least 1 class
        self.num_classes = max(1, int(num_classes))
        base = [
            ("good movie great acting", 1 if self.num_classes > 1 else 0),
            ("bad plot boring scenes", 0),
            ("excellent visuals and story", 1 if self.num_classes > 1 else 0),
            ("terrible pacing and sound", 0),
        ]
        self.data = (base * ((self.length // 4) + 1))[: self.length]

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        text, y = self.data[idx]
        y = int(y) % self.num_classes  # defensive mod
        if self.tokenizer is None:
            x = torch.randn(768)
            return {"inputs": x, "labels": torch.tensor(y, dtype=torch.long)}
        enc = self.tokenizer(
            text,
            padding='max_length',
            truncation=True,
            max_length=self.max_length,
            return_tensors='pt',
        )
        enc = {k: v.squeeze(0) for k, v in enc.items()}
        return {"inputs": enc, "labels": torch.tensor(y, dtype=torch.long)}


class TinyImageDataset(Dataset):
    def __init__(self, size=224, num_classes: int = NUM_CLASSES, length=32):
        self.size = int(size)
        self.length = int(length)
        self.num_classes = max(1, int(num_classes))

    def __len__(self):
        return self.length

    def __getitem__(self, idx):
        x = torch.rand(3, self.size, self.size)
        if self.num_classes <= 1:
            y = 0
        else:
            y = torch.randint(0, self.num_classes, (1,)).item()
        return {"inputs": x, "labels": torch.tensor(y, dtype=torch.long)}


def build_dataloaders(config_path: str = None):
    TASK = "{{ (ir.get('model') or {}).get('task_type', '') | lower }}"
    DOMAIN = "{{ (ir.get('domain') or '') | lower }}"
    is_text = ("nlp" in DOMAIN) or ("sequence" in TASK) or ("text" in TASK)

    try:
        batch_size = int((ir.get('hyperparameters') or {}).get('batch_size', 8))
    except Exception:
        batch_size = 8
    batch_size = max(1, min(batch_size, 8))

    tokenizer = None
    if is_text and AutoTokenizer is not None:
        try:
            tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
        except Exception:
            tokenizer = None

    if is_text:
        ds_train = TinyTextDataset(tokenizer=tokenizer, num_classes=NUM_CLASSES)
        ds_val = TinyTextDataset(tokenizer=tokenizer, num_classes=NUM_CLASSES)
    else:
        ds_train = TinyImageDataset(num_classes=NUM_CLASSES)
        ds_val = TinyImageDataset(num_classes=NUM_CLASSES)

    train_loader = DataLoader(ds_train, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(ds_val, batch_size=batch_size)
    return train_loader, val_loader
